{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing \n",
    "\n",
    "The following notebook keep tweets in spanish, remove accents, and other data engineering steps to create the data for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language detection \n",
    "import langid  \n",
    "from langdetect import detect  \n",
    "import textblob\n",
    "\n",
    "# Data manipulation and other\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current and data working directory\n",
    "cwd = os.getcwd()\n",
    "dwd = cwd + \"\\\\data_csv\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv data sets\n",
    "bolP  = pd.read_csv(dwd + \"querybol_p.csv\"   , low_memory = False)\n",
    "bolNP = pd.read_csv(dwd + \"querybol_np.csv\"  , low_memory = False)\n",
    "chiP  = pd.read_csv(dwd + \"querychil.csv\"    , low_memory = False)\n",
    "chiNP = pd.read_csv(dwd + \"querychil_np.csv\" , low_memory = False)\n",
    "colP  = pd.read_csv(dwd + \"querycol_p.csv\"   , low_memory = False)\n",
    "colNP = pd.read_csv(dwd + \"querycol_nonp.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to evaluate language\n",
    "# Source: http://blog.manugarri.com/sentiment-analysis-in-spanish/\n",
    "\n",
    "def langid_safe(tweet):  \n",
    "    try:\n",
    "        return langid.classify(tweet)[0]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "def langdetect_safe(tweet):  \n",
    "    try:\n",
    "        return detect(tweet)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "def textblob_safe(tweet):  \n",
    "    try:\n",
    "        return textblob.TextBlob(tweet).detect_language()\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Diss 30nov passejarem per les ribes del Brugen...\n",
       "1    @IvanDuque el tocino sigue asesinando su puebl...\n",
       "2    A ver si lo entienden quienes pretenden vender...\n",
       "3    Luisa Amanda ha padecido las penurias de mante...\n",
       "4    #29NParoNacional una manera de protestar es no...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create temp data to test language detect functions\n",
    "temp = colP.iloc[:5].copy()\n",
    "colP.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.5 ms ± 1.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "27.7 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "319 ms ± 20.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Temp data to evaluate time per function \n",
    "%timeit temp['lang_langid']     = temp.text.apply(langid_safe)  \n",
    "%timeit temp['lang_langdetect'] = temp.text.apply(langdetect_safe)  \n",
    "%timeit temp['lang_textblob']   = temp.text.apply(textblob_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of dataframes\n",
    "pdlist = [bolP, bolNP, chiP, chiNP, colP, colNP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create language id \n",
    "for data in pdlist:\n",
    "    data['lang_langid'] = data.text.apply(langid_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep tweets only in spanish\n",
    "bolP  = bolP [bolP.lang_langid  == \"es\"]\n",
    "bolNP = bolNP[bolNP.lang_langid == \"es\"]\n",
    "chiP  = chiP [chiP.lang_langid  == \"es\"]\n",
    "chiNP = chiNP[chiNP.lang_langid == \"es\"]\n",
    "colP  = colP [colP.lang_langid  == \"es\"]\n",
    "colNP = colNP[colNP.lang_langid == \"es\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate how balance are the datasets\n",
    "bolP.shape, bolNP.shape, chiP.shape, chiNP.shape, colP.shape, colNP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data engineering (continuation)\n",
    "\n",
    "1. Create protest and country id\n",
    "2. Append datasets\n",
    "3. Keep variables of interest\n",
    "4. Create clean text with hashtags and mentioned users\n",
    "5. Remove accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create protest and country id \n",
    "bolP [\"country\"], bolP [\"protest\"] = [\"bol\",1]\n",
    "bolNP[\"country\"], bolNP[\"protest\"] = [\"bol\",0]\n",
    "chiP [\"country\"], chiP [\"protest\"] = [\"chi\",1] \n",
    "chiNP[\"country\"], chiNP[\"protest\"] = [\"chi\",0]\n",
    "colP [\"country\"], colP [\"protest\"] = [\"col\",1] \n",
    "colNP[\"country\"], colNP[\"protest\"] = [\"col\",0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append datasets\n",
    "tweetslat = bolP.copy()\n",
    "tweetslat = tweetslat.append(bolNP)\n",
    "tweetslat = tweetslat.append(chiP)\n",
    "tweetslat = tweetslat.append(chiNP)\n",
    "tweetslat = tweetslat.append(colP)\n",
    "tweetslat = tweetslat.append(colNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetslat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variable of interest\n",
    "tweetslat = tweetslat[['user_id','country','protest',\n",
    "                       'timestamp','text','hashtags','mentioned_users',\n",
    "                       'likes','retweets',\n",
    "                       'cleaned_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String variables to lowercase \n",
    "tweetslat['hashtags']        = tweetslat['hashtags'].apply(str.lower)\n",
    "tweetslat['mentioned_users'] = tweetslat['mentioned_users'].apply(str.lower)\n",
    "\n",
    "# Remove @ on mentioned users\n",
    "tweetslat['mentioned_users'] = tweetslat['mentioned_users'].str.replace('@','')\n",
    "\n",
    "# Hashtags, mentioned_users and cleaned_test from str list to list\n",
    "tweetslat['hashtags']        = tweetslat['hashtags'].str.replace(\"'\",\"\").str.strip(\"][\").str.split(\",\")\n",
    "tweetslat['mentioned_users'] = tweetslat['mentioned_users'].str.replace(\"'\",\"\").str.strip(\"][\").str.split(\",\")\n",
    "tweetslat['cleaned_text']    = tweetslat['cleaned_text'].str.replace(\"'\",\"\").str.strip(\"][\").str.split(\",\")\n",
    "\n",
    "# Merge cleaned_text, hashtags and mentioned_users\n",
    "tweetslat['clean_text_2']    = tweetslat['hashtags'] + tweetslat['mentioned_users'] + tweetslat['cleaned_text']\n",
    "\n",
    "# Remove empty elements and accents\n",
    "tweetslat['clean_text_2']    = tweetslat['clean_text_2'].apply(lambda x: [unidecode.unidecode(i) for i in x if len(i) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetslat.to_csv(dwd + \"tweetslat.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
